# ============================================================
# combined_inference.py â€” å…©éšæ®µæ¨è«–ï¼š
#   1) å½±åƒæ¨¡å‹ + å•å· + DeepSeek (åˆæ­¥å¯èƒ½è¨ºæ–·)
#   2) æ ¹æ“šå€™é¸è¨ºæ–·åš RAG + DeepSeek (æœ€çµ‚æ•´åˆ)
# ============================================================

import json
from typing import Dict, Any, List, Optional

import requests

from inference import predict_image
from lesion_inference import predict_lesion
from rag_milvus import search_knowledge

# æƒ¡æ€§ç—…è®Šç¨®é¡
MALIGNANT = {"MEL", "BCC", "AKIEC"}

# Ollama ä¼ºæœå™¨ï¼ˆDeepSeek-R1 14Bï¼‰
OLLAMA_URL = "http://127.0.0.1:11434/api/generate"
LLM_MODEL = "deepseek-r1:14b"


# ------------------------------------------------------------
# å·¥å…·ï¼šå¾ SwinV2 çµæœä¸­åªå–ä¸€å€‹ lesion top1ï¼ˆå¾ lesions æ’åºï¼‰
# ------------------------------------------------------------
def get_lesion_top1(lesion: Dict[str, Any]) -> Dict[str, Any]:
    lesions = lesion.get("lesions") or []
    if not lesions:
        return {}

    # å¾ lesions è£¡ç”¨ confidence æ’åºå–ç¬¬ä¸€å€‹
    top1 = max(lesions, key=lambda x: x.get("confidence", 0.0))
    return {
        "label": top1.get("label"),
        "confidence": float(top1.get("confidence", 0.0)),
    }


# ------------------------------------------------------------
# DeepSeek ç¬¬ä¸€éšæ®µ Promptï¼šä¸å« RAGï¼Œåªçœ‹ç—‡ç‹€ + å½±åƒè¨Šæ¯
# ------------------------------------------------------------
def build_first_prompt(
    disease: Dict[str, Any],
    lesion_top1: Dict[str, Any],
    survey: Dict[str, Any],
) -> str:
    sb: List[str] = []

    sb.append("ä½ æ˜¯ä¸€ä½å”åŠ©å°ç£çš®è†šç§‘é–€è¨ºçš„è‡¨åºŠè¼”åŠ©ç³»çµ±ã€‚")
    sb.append("åœ¨ç¬¬ä¸€éšæ®µï¼Œä½ åªèƒ½æ ¹æ“šï¼š")
    sb.append("1. æ‚£è€…å¡«å¯«çš„ç—‡ç‹€èˆ‡ç—…ç¨‹å•å·")
    sb.append("2. å½±åƒæ¨¡å‹æä¾›çš„å¤–è§€èˆ‡ç—…è®Šè³‡è¨Šï¼ˆåƒ…ä½œåƒè€ƒï¼Œä¸å¯å®Œå…¨ä¾è³´ï¼‰")
    sb.append("")
    sb.append("âš ï¸ æ­¤éšæ®µã€Œå¯ä»¥ã€å¼•ç”¨ä»»ä½•å¤–éƒ¨é†«å­¸è³‡æ–™æˆ–æ•™ç§‘æ›¸å…§å®¹ï¼Œä¸éœ€åªæ ¹æ“šæˆ‘çµ¦çš„è³‡è¨Šåšåˆæ­¥æ¨è«–ã€‚")
    sb.append("è«‹è¼¸å‡º JSON æ ¼å¼ï¼Œä¸è¦åŠ è¨»è§£æˆ–å¤šé¤˜æ–‡å­—ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š")
    sb.append(
        """
{
  "candidates": ["ç–¾ç—…åç¨±1", "ç–¾ç—…åç¨±2", "..."],
  "reasoning": "ä½ æ ¹æ“šå•å·èˆ‡å¤–è§€åšå‡ºçš„æ¨ç†èªªæ˜ï¼ˆç¹é«”ä¸­æ–‡ï¼‰"
}
        """.strip()
    )
    sb.append("")
    sb.append("=== å½±åƒæ¨¡å‹çµæœï¼ˆåƒ…ä¾›åƒè€ƒï¼‰ ===")

    if disease:
        sb.append(
            f"- ç–¾ç—…æ¨¡å‹ top1ï¼š{disease.get('class_name', 'æœªçŸ¥')} "
            f"(ä¿¡å¿ƒ {disease.get('confidence', 0.0)*100:.1f}%)"
        )
        top3 = disease.get("top3") or []
        if top3:
            sb.append("- ç–¾ç—…æ¨¡å‹ top3ï¼š")
            for i, item in enumerate(top3, start=1):
                sb.append(
                    f"  {i}. {item.get('label', 'æœªçŸ¥')} "
                    f"({item.get('confidence', 0.0)*100:.1f}%)"
                )

    if lesion_top1:
        sb.append(
            f"- ç—…è®Šæ¨¡å‹ä¸»è¦ç‰¹å¾µï¼š{lesion_top1.get('label', 'æœªçŸ¥')} "
            f"(ä¿¡å¿ƒ {lesion_top1.get('confidence', 0.0)*100:.1f}%)"
        )

    sb.append("")
    sb.append("=== æ‚£è€…å•å·å…§å®¹ï¼ˆåŸå§‹ JSONï¼‰ ===")
    sb.append(json.dumps(survey, ensure_ascii=False, indent=2))

    sb.append("")
    sb.append("è«‹æ ¹æ“šä»¥ä¸Šè³‡è¨Šï¼Œç”¢å‡ºæœ€æœ‰å¯èƒ½çš„ 3~5 å€‹çš®è†šç–¾ç—…å€™é¸ï¼ˆå¸¸è¦‹åç¨±å³å¯ï¼‰ï¼Œ")
    sb.append("ä¸¦ä»¥ JSON å›è¦†ï¼ˆåªå…è¨±ä¸Šè¿°æ¬„ä½ï¼‰ã€‚")

    return "\n".join(sb)


# ------------------------------------------------------------
# DeepSeek ç¬¬äºŒéšæ®µ Promptï¼šæ•´åˆ RAG + ç¬¬ä¸€éšæ®µçµæœ
# ------------------------------------------------------------
def build_second_prompt(
    disease: Dict[str, Any],
    lesion_top1: Dict[str, Any],
    survey: Dict[str, Any],
    candidates: List[str],
    first_reasoning: str,
    rag_info: List[Dict[str, Any]],
    model_summary: str,
) -> str:
    sb: List[str] = []

    sb.append("ä½ æ˜¯ä¸€å¥—å°ç£çš®è†šç§‘è‡¨åºŠè¼”åŠ©ç³»çµ±ï¼Œç¾åœ¨é€²å…¥ç¬¬äºŒéšæ®µï¼š")
    sb.append("å¯ä»¥ä½¿ç”¨ä¸‹åˆ—ä¾†æºä½œç‚ºåˆ¤æ–·ä¾æ“šï¼ˆä¾é‡è¦æ€§æ’åºï¼‰ï¼š")
    sb.append("1. æ‚£è€…å•å·èˆ‡è‡ªè¿°")
    sb.append("2. RAG æä¾›çš„çš®è†šç§‘é†«å­¸å…§å®¹ï¼ˆå”¯ä¸€å¯é é†«ç™‚çŸ¥è­˜ä¾†æºï¼‰")
    sb.append("3. ç¬¬ä¸€éšæ®µæ¨è«–èˆ‡å½±åƒæ¨¡å‹çµæœï¼ˆåƒ…ä½œè¼”åŠ©ï¼Œä¸å¾—å‡Œé§•ç—‡ç‹€èˆ‡ RAGï¼‰")
    sb.append("")
    sb.append("âš ï¸ åš´æ ¼è¦å‰‡ï¼š")
    sb.append("ï¼ç¦æ­¢ä½¿ç”¨ä½ è‡ªèº«çš„é†«å­¸çŸ¥è­˜ï¼Œåªèƒ½å¼•ç”¨æˆ‘æä¾›çš„ RAG æ–‡å­—å…§å®¹ã€‚")
    sb.append("ï¼è‹¥æŸç–¾ç—…æœªå‡ºç¾åœ¨ RAG ä¸­ï¼Œä¸è¦éåº¦å»¶ä¼¸ï¼Œåªèƒ½èªªã€è³‡è¨Šä¸è¶³ã€ã€‚")
    sb.append("ï¼é¿å…ä½¿ç”¨è—¥å“å•†å“åï¼Œåªèƒ½æåˆ°ã€å«æœ‰æŸæˆåˆ†çš„å¤–ç”¨è—¥ã€ä¹‹é¡æè¿°ã€‚")
    sb.append("ï¼è¼¸å‡ºå…¨éƒ¨ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œèªæ°£ä¸­ç«‹ã€å°ˆæ¥­ã€æ˜“æ‡‚ã€‚")
    sb.append("")
    sb.append("=== ç¬¬ä¸€éšæ®µæ¨è«–ï¼ˆä¾†è‡ª DeepSeekï¼‰ ===")
    sb.append("å€™é¸ç–¾ç—…åˆ—è¡¨ï¼š")
    for i, c in enumerate(candidates, start=1):
        sb.append(f"{i}. {c}")
    sb.append("")
    sb.append("ç¬¬ä¸€éšæ®µæ¨ç†æ‘˜è¦ï¼š")
    sb.append(first_reasoning or "ï¼ˆç„¡ï¼‰")
    sb.append("")
    sb.append("=== å½±åƒæ¨¡å‹æ‘˜è¦ï¼ˆåƒ…ä¾›å¼±åƒè€ƒï¼‰ ===")
    sb.append(model_summary)
    if lesion_top1:
        sb.append(
            f"ç—…è®Šä¸»è¦ç‰¹å¾µï¼š{lesion_top1.get('label', 'æœªçŸ¥')} "
            f"(ä¿¡å¿ƒ {lesion_top1.get('confidence', 0.0)*100:.1f}%)"
        )
    sb.append("")
    sb.append("=== æ‚£è€…å•å·å…§å®¹ï¼ˆJSONï¼‰ ===")
    sb.append(json.dumps(survey, ensure_ascii=False, indent=2))

    sb.append("")
    sb.append("=== RAG é†«å­¸è³‡æ–™ï¼ˆä½ å”¯ä¸€èƒ½å¼•ç”¨çš„é†«ç™‚çŸ¥è­˜ï¼‰ ===")
    if not rag_info:
        sb.append("ï¼ˆæœªæ‰¾åˆ°ç›¸é—œ RAG è³‡æ–™ï¼Œå¦‚è³‡è¨Šä¸è¶³è«‹æ¸…æ¥šèªªæ˜ä¸ç¢ºå®šæ€§ï¼‰")
    else:
        for i, item in enumerate(rag_info, start=1):
            title = item.get("title") or "æœªå‘½åæ¢ç›®"
            content = item.get("content") or ""
            url = item.get("url") or ""
            sb.append(f"ã€è³‡æ–™ {i}ï¼š{title}ã€‘")
            if url:
                sb.append(f"ä¾†æºé€£çµï¼š{url}")
            sb.append(content)
            sb.append("")

    sb.append("")
    sb.append("=== è«‹ä¾ä¸‹åˆ—çµæ§‹è¼¸å‡ºæœ€çµ‚è©•ä¼°ï¼ˆç¹é«”ä¸­æ–‡ï¼‰ ===")
    sb.append("ä¸€ã€å¯èƒ½è¨ºæ–·ï¼š")
    sb.append("ï¼åˆ—å‡º 2â€“4 å€‹æœ€å¯èƒ½çš„è¨ºæ–·ï¼Œä¸¦èªªæ˜ç—‡ç‹€èˆ‡ RAG å“ªäº›éƒ¨åˆ†æ”¯æŒè©²è¨ºæ–·ã€‚")
    sb.append("")
    sb.append("äºŒã€é‘‘åˆ¥è¨ºæ–·ï¼š")
    sb.append("ï¼èªªæ˜å¹¾å€‹éœ€è¦å€åˆ†çš„å…¶ä»–ç–¾ç—…ï¼Œå¼·èª¿å¤–è§€ï¼åˆ†ä½ˆï¼ç—…ç¨‹ä¸Šçš„å·®ç•°ã€‚")
    sb.append("")
    sb.append("ä¸‰ã€å±…å®¶ç…§è­·èˆ‡æ—¥å¸¸å»ºè­°ï¼š")
    sb.append("ï¼æä¾›æ¸…æ½”ã€ä¿æ¿•ã€é¿å…åˆºæ¿€èˆ‡ç”Ÿæ´»ä½œæ¯å»ºè­°ã€‚")
    sb.append("ï¼è‹¥æåˆ°ç”¨è—¥ï¼Œåªèƒ½æè¿°ã€å«æœ‰æŸæˆåˆ†çš„å¤–ç”¨è—¥ç‰©ã€ï¼Œä¸å¾—å¯«å•†å“åã€‚")
    sb.append("")
    sb.append("å››ã€å°±é†«èˆ‡è­¦è¨Šï¼š")
    sb.append("ï¼èªªæ˜ä»€éº¼æƒ…æ³ä¸‹æ‡‰å„˜é€Ÿå°±é†«ï¼Œå¦‚å¿«é€Ÿæƒ¡åŒ–ã€æ»²æ¶²ã€åŠ‡ç—›ã€è‡‰éƒ¨æˆ–ç”Ÿæ®–éƒ¨ä½ç—…ç¶ç­‰ã€‚")
    sb.append("ï¼è‹¥æ‡·ç–‘æœ‰æƒ¡æ€§ç—…è®Šå¯èƒ½ï¼Œéœ€æ˜ç¢ºæ¨™è¨»ä¸¦æé†’å°±é†«ã€‚")

    return "\n".join(sb)


# ------------------------------------------------------------
# DeepSeek å‘¼å«å·¥å…·
# ------------------------------------------------------------
def call_llm(prompt: str) -> str:
    payload = {
        "model": LLM_MODEL,
        "prompt": prompt,
        "stream": False,
        "temperature": 0.7,
    }
    resp = requests.post(OLLAMA_URL, json=payload)
    resp.raise_for_status()
    data = resp.json()
    # Ollama /generate é è¨­æ¬„ä½å« "response"
    return data.get("response", "")


# ------------------------------------------------------------
# ä¸»æµç¨‹ï¼šå…©éšæ®µæ¨è«– + RAG
# ------------------------------------------------------------
def predict_combined(
    image_path: str,
    survey: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    if survey is None:
        survey = {}

    try:
        # 1ï¸âƒ£ å½±åƒæ¨¡å‹ï¼šç–¾ç—…åˆ†é¡
        disease = predict_image(image_path)

        # 2ï¸âƒ£ å½±åƒæ¨¡å‹ï¼šç—…è®Šåˆ†é¡ï¼ˆSwinV2ï¼‰
        lesion = predict_lesion(image_path)
        lesion_top1 = get_lesion_top1(lesion)
        lesion["top1"] = lesion_top1  # è£œå……å¯«å›å»æ–¹ä¾¿å‰ç«¯æˆ–æ—¥å¾Œä½¿ç”¨

        # 3ï¸âƒ£ æƒ¡æ€§é¢¨éšªåˆ†æï¼ˆæ²¿ç”¨ä½ åŸæœ¬é‚è¼¯ï¼‰
        risk_flag = "ğŸŸ¢ è‰¯æ€§å¯èƒ½æ€§é«˜"
        for item in lesion.get("top3", []):
            label = item.get("label", "")
            conf = item.get("confidence", 0)
            if label in MALIGNANT:
                if conf >= 0.85:
                    risk_flag = "ğŸ”´ é«˜åº¦æ‡·ç–‘æƒ¡æ€§ï¼Œå»ºè­°ç›¡é€Ÿå°±é†«"
                elif conf >= 0.70:
                    risk_flag = "ğŸŸ¡ ç—…ç¶æœ‰ç–‘ä¼¼æƒ¡æ€§ç‰¹å¾µï¼Œå»ºè­°è§€å¯Ÿæˆ–å°±é†«"
                else:
                    risk_flag = "ğŸŸ¢ ç„¡æ˜é¡¯æƒ¡æ€§ç‰¹å¾µ"

        # 4ï¸âƒ£ æ¨¡å‹ç«¯æ‘˜è¦ï¼ˆçµ¦ç¬¬äºŒéšæ®µç”¨ï¼‰
        lesion_names = [x["label"] for x in lesion.get("top3", [])]
        summary = (
            f"ç–¾ç—…æ¨¡å‹é æ¸¬ç‚º {disease['class_name']}ï¼ˆä¿¡å¿ƒ {disease['confidence']*100:.1f}%ï¼‰ï¼›"
            f"ç—…è®Šæ¨¡å‹åµæ¸¬åˆ°ä¸»è¦ç‰¹å¾µï¼š{', '.join(lesion_names) or 'ç„¡'}ã€‚"
        )

        # ===============================
        #   ç¬¬ä¸€éšæ®µ DeepSeekï¼šå€™é¸è¨ºæ–·
        # ===============================
        first_prompt = build_first_prompt(disease, lesion_top1, survey)
        first_raw = call_llm(first_prompt)

        # è§£æç¬¬ä¸€éšæ®µ JSON
        candidates: List[str] = []
        reasoning = ""
        try:
            first_json = json.loads(first_raw)
            if isinstance(first_json, dict):
                c_list = first_json.get("candidates") or []
                candidates = [str(x) for x in c_list if isinstance(x, str)]
                reasoning = str(first_json.get("reasoning") or "")
        except Exception:
            # è§£æå¤±æ•—å°± fallbackï¼šç”¨ç–¾ç—…æ¨¡å‹ class_name ç•¶å”¯ä¸€å€™é¸
            candidates = [disease.get("class_name", "")]
            reasoning = first_raw

        if not candidates:
            candidates = [disease.get("class_name", "")]

        # ===============================
        #   ç¬¬äºŒéšæ®µï¼šRAG + DeepSeek
        # ===============================
        # å°æ¯å€‹å€™é¸è¨ºæ–·åš RAG æœå°‹
        rag_info: List[Dict[str, Any]] = []
        seen_keys = set()
        for c in candidates:
            if not c:
                continue
            results = search_knowledge(c, top_k=3)
            for r in results:
                title = r.get("title") or ""
                url = r.get("url") or ""
                key = (title, url)
                if key in seen_keys:
                    continue
                seen_keys.add(key)
                rag_info.append(r)

        second_prompt = build_second_prompt(
            disease=disease,
            lesion_top1=lesion_top1,
            survey=survey,
            candidates=candidates,
            first_reasoning=reasoning,
            rag_info=rag_info,
            model_summary=summary,
        )
        final_text = call_llm(second_prompt)

        # çµ±ä¸€ä¸€äº›è®Šæ•¸åç¨±çµ¦ return ç”¨
        rag_results = rag_info
        final_candidates = candidates
        final_top1 = final_candidates[0] if final_candidates else "ç„¡æ³•åˆ¤å®š"

        # æœ€å¾Œçµ±ä¸€å›å‚³æ ¼å¼
        return {
            "disease": disease,        # åŸå§‹ç–¾ç—…æ¨¡å‹çµæœ
            "lesion": lesion,          # åŸå§‹ç—…è®Šæ¨¡å‹çµæœï¼ˆå« top1ï¼‰
            "lesion_top1": lesion_top1,
            "rag": rag_results,        # RAG æ®µè½
            "risk_flag": risk_flag,
            "summary": summary,

            # ç¬¬ä¸€éšæ®µ debug è³‡è¨Šï¼ˆä¿ç•™çµ¦ä½ çœ‹ç”¨ï¼‰
            "first_pass": {
                "raw_response": first_raw,
                "candidates": candidates,
                "reasoning": reasoning,
            },

            # âœ… é€™ä¸‰å€‹æ˜¯çµ¦ Flutter ç”¨çš„æ–° API
            "final_top1": final_top1,
            "final_candidates": final_candidates,
            "final_text": final_text,
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {
            "error": str(e),
            "summary": "âš ï¸ æ¨è«–éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤",
            "rag": [],
            "final_text": "ç³»çµ±åœ¨åˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦æˆ–æ´½ç³»çµ±ç®¡ç†è€…ã€‚",
        }
