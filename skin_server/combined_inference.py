# combined_inference.py
# å–®éšæ®µæµç¨‹ï¼š
#   ConvNeXt å½±åƒåˆ†é¡ â†’ RAGï¼ˆMilvusï¼‰â†’ DeepSeek LLM å ±å‘Š

import json
from typing import Dict, Any, List, Optional

import requests

from lesion_model import predict_lesion
from rag_milvus import search_knowledge

# Ollama ä¼ºæœå™¨ï¼ˆDeepSeek-R1 14Bï¼‰
OLLAMA_URL = "http://127.0.0.1:11434/api/generate"
LLM_MODEL = "deepseek-r1:14b"


# ---------------------------
# é¢¨éšªè©•ä¼°ï¼ˆä¾ç›®å‰ 8 é¡ä¸­æ–‡æ¨™ç±¤ï¼‰
# ---------------------------
def compute_risk_flag(top1_label: str, conf: float) -> str:
    """
    ä¾ç…§åˆ†é¡çµæœçµ¦ä¸€å€‹å¤§æ¦‚çš„é¢¨éšªç­‰ç´šå­—ä¸²ã€‚
    ä½ ä¹‹å¾Œè¦ºå¾—ä¸åˆç†å¯ä»¥å†èª¿é€™è£¡å°±å¥½ã€‚
    """
    malignant = {"åŸºåº•ç´°èƒç™Œ", "é±—ç‹€ç´°èƒç™Œ"}
    pre_malignant = {"å…‰åŒ–æ€§è§’åŒ–"}

    if top1_label in malignant and conf >= 0.50:
        return "ğŸ”´ å½±åƒé¡¯ç¤ºç–‘ä¼¼çš®è†šæƒ¡æ€§è…«ç˜¤ï¼Œå»ºè­°å„˜é€Ÿå°±é†«ï¼Œç”±çš®è†šç§‘é†«å¸«é¢è¨ºç¢ºèªã€‚"
    if top1_label in pre_malignant and conf >= 0.50:
        return "ğŸŸ¡ å½±åƒé¡¯ç¤ºå¯èƒ½ç‚ºç™Œå‰ç—…è®Šï¼Œå»ºè­°å„˜æ—©å®‰æ’çš®è†šç§‘é–€è¨ºè¿½è¹¤ã€‚"
    return "ğŸŸ¢ ç›®å‰åˆ†é¡å¤šåå‘è‰¯æ€§ç—…ç¶ï¼Œä½†ä»å»ºè­°ä¾å¯¦éš›ç—‡ç‹€èˆ‡é†«å¸«è©•ä¼°ç‚ºä¸»ã€‚"


# ---------------------------
# LLM å ±å‘Šç”¨çš„ Prompt
# ---------------------------
def build_report_prompt(
    lesion: Dict[str, Any],
    rag_info: List[Dict[str, Any]],
    risk_flag: str,
) -> str:
    top1 = lesion.get("top1", {})
    top3 = lesion.get("top3", [])

    label = top1.get("label", "æœªçŸ¥ç—…ç¶")
    conf = float(top1.get("confidence", 0.0)) * 100.0

    sb: List[str] = []

    sb.append("ä½ æ˜¯ä¸€å¥—å”åŠ©å°ç£çš®è†šç§‘é–€è¨ºçš„è‡¨åºŠæ±ºç­–è¼”åŠ©ç³»çµ±ã€‚")
    sb.append("ä½ åªèƒ½æ ¹æ“šæˆ‘æä¾›çš„ RAG é†«å­¸å…§å®¹èˆ‡æ¨¡å‹è¼¸å‡ºé€²è¡Œèªªæ˜ï¼Œä¸å¯ä»¥è‡ªè¡Œå»¶ä¼¸é¡å¤–é†«å­¸çŸ¥è­˜ã€‚")
    sb.append("è«‹ç”¨ç¹é«”ä¸­æ–‡ï¼Œèªæ°£ä¸­ç«‹ä¸”æ˜“æ‡‚ï¼Œå›ç­”çµ¦ä¸€èˆ¬æ°‘çœ¾é–±è®€ã€‚")
    sb.append("")
    sb.append("=== å½±åƒ AI åˆ†é¡çµæœï¼ˆåƒ…ä¾›åƒè€ƒï¼Œéæ­£å¼è¨ºæ–·ï¼‰ ===")
    sb.append(f"- æ¨¡å‹ä¸»è¦åˆ†é¡çµæœï¼š{label}ï¼ˆä¿¡å¿ƒç´„ {conf:.1f}%ï¼‰")
    if top3:
        sb.append("- Top3 å¯èƒ½çµæœï¼š")
        for i, item in enumerate(top3, start=1):
            sb.append(
                f"  {i}. {item.get('label', 'æœªçŸ¥')} "
                f"(ç´„ {float(item.get('confidence', 0.0))*100:.1f}%)"
            )
    sb.append("")
    sb.append("=== é¢¨éšªæç¤ºï¼ˆç³»çµ±å…§è¦å‰‡è©•ä¼°ï¼‰ ===")
    sb.append(risk_flag)
    sb.append("")
    sb.append("=== å¯ä½¿ç”¨çš„é†«å­¸çŸ¥è­˜ï¼ˆRAG æŸ¥è©¢çµæœï¼‰ ===")

    if not rag_info:
        sb.append("ï¼ˆç›®å‰è³‡æ–™åº«ä¸­æ²’æœ‰æ‰¾åˆ°èˆ‡æ­¤ç—…åç›¸ç¬¦çš„æ¢ç›®ï¼Œè«‹ä½ æ˜ç¢ºèªªæ˜è³‡è¨Šæœ‰é™ã€‚ï¼‰")
    else:
        for i, item in enumerate(rag_info, start=1):
            title = item.get("title") or "æœªå‘½åæ¢ç›®"
            content = item.get("content") or ""
            url = item.get("url") or ""
            sb.append(f"ã€è³‡æ–™ {i}ï¼š{title}ã€‘")
            if url:
                sb.append(f"ä¾†æºé€£çµï¼š{url}")
            sb.append(content)
            sb.append("")

    sb.append("")
    sb.append("=== å›è¦†è¦æ±‚ ===")
    sb.append("è«‹ä¾ç…§ä¸‹é¢çµæ§‹ï¼Œæ•´ç†ä¸€ä»½çµ¦æ°‘çœ¾çœ‹çš„èªªæ˜ï¼š")
    sb.append("ä¸€ã€æ­¤é¡çš®è†šç—…ç¶çš„ç°¡ä»‹ï¼ˆä¾ç…§ RAG å…§å®¹ï¼Œä¸è¦åŠ å…¥é¡å¤–çŸ¥è­˜ï¼‰ã€‚")
    sb.append("äºŒã€å¸¸è¦‹çš„ç—‡ç‹€èˆ‡å¤–è§€ç‰¹å¾µï¼ˆç›¡é‡å°æ‡‰ RAG å…§å®¹ï¼‰ã€‚")
    sb.append("ä¸‰ã€å¯èƒ½çš„é¢¨éšªèˆ‡éœ€è¦æ³¨æ„çš„æƒ…æ³ï¼ˆçµåˆä¸Šè¿°é¢¨éšªæç¤ºèˆ‡ RAGï¼‰ã€‚")
    sb.append("å››ã€å±…å®¶ç…§è­·èˆ‡æ—¥å¸¸æ³¨æ„äº‹é …ï¼ˆæ¸…æ½”ã€ä¿æ¿•ã€é¿å…åˆºæ¿€ç­‰ï¼Œä¸€æ¨£è¦ä»¥ RAG å…§å®¹ç‚ºä¸»ï¼‰ã€‚")
    sb.append("äº”ã€ä½•æ™‚æ‡‰è©²å°±é†«æˆ–å›è¨ºï¼Œç‰¹åˆ¥æ˜¯å“ªäº›è­¦è¨Šéœ€è¦å„˜é€Ÿå°±é†«ã€‚")
    sb.append("")
    sb.append("è«‹ä»¥æ¢åˆ—èˆ‡çŸ­æ®µè½æ•´ç†ï¼Œè®“ä¸€èˆ¬æ°‘çœ¾å¯ä»¥çœ‹æ‡‚ã€‚")

    return "\n".join(sb)


# ---------------------------
# å‘¼å« Ollama LLM
# ---------------------------
def call_llm(prompt: str) -> str:
    payload = {
        "model": LLM_MODEL,
        "prompt": prompt,
        "stream": False,
        "temperature": 0.7,
    }
    resp = requests.post(OLLAMA_URL, json=payload, timeout=300)
    resp.raise_for_status()
    data = resp.json()
    return data.get("response", "")


# ---------------------------
# ä¸»æµç¨‹ï¼šå½±åƒ â†’ RAG â†’ LLM
# ---------------------------
def predict_combined(image_path: str, survey=None):
    try:
        print("\n==============================")
        print("ğŸ”¥ [COMBINED] å½±åƒ â†’ RAG â†’ LLM é–‹å§‹")
        print("==============================")

        # 1ï¸âƒ£ æ¨¡å‹åˆ†é¡
        lesion = predict_lesion(image_path)
        top1 = lesion.get("top1", {})
        label = top1.get("label", "æœªçŸ¥")
        conf = float(top1.get("confidence", 0.0))

        print(f"ğŸ” [Model] Top1 = {label} ({conf*100:.1f}%)")

        # 2ï¸âƒ£ RAG æŸ¥è©¢
        rag_info = search_knowledge(label, top_k=5)

        print("\n===== ğŸ”µ [RAG Query] =====")
        if not rag_info:
            print("âš ï¸ RAG æ²’å–åˆ°ä»»ä½•ç›¸é—œçŸ¥è­˜ï¼ˆå¯èƒ½æ˜¯æ¨™ç±¤åç¨±å°ä¸ä¸Šè³‡æ–™åº«ï¼‰")
        else:
            print(f"ğŸŸ¢ å…±å–åˆ° {len(rag_info)} ç­† RAG è³‡æ–™")
            for i, item in enumerate(rag_info, start=1):
                print(f"  RAG {i}: {item.get('title')} (å­—æ•¸ {len(item.get('content',''))})")

        # 3ï¸âƒ£ å»ºç«‹ Prompt + å‘¼å« LLM
        prompt = build_report_prompt(lesion, rag_info, compute_risk_flag(label, conf))

        try:
            print("\n===== ğŸ¤– å‘¼å« DeepSeek =====")
            final_text = call_llm(prompt)
            print(f"âœ¨ LLM å›æ‡‰å­—æ•¸ï¼š{len(final_text)}")
        except Exception as err:
            print("âŒ LLM å‘¼å«å¤±æ•—ï¼š", err)
            final_text = "ï¼ˆLLM å›æ‡‰å¤±æ•—ï¼Œä½†åˆ†é¡çµæœå·²ç”¢ç”Ÿï¼‰"

        return {
            "final_top1": label,
            "final_text": final_text,
            "rag": rag_info,
            "lesion": lesion,
        }

    except Exception as e:
        print("âŒ COMBINED ERROR:", e)
        return {
            "final_top1": "æœªçŸ¥",
            "final_text": "ç³»çµ±ç™¼ç”ŸéŒ¯èª¤",
            "rag": [],
            "lesion": {},
        }
